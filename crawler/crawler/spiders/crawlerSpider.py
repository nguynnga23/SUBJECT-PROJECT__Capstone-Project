import scrapy
import json
from urllib.parse import urljoin
from datetime import datetime
from crawler.items import ArticleItem
from scrapy.exceptions import CloseSpider
import requests
from crawler.config import UNIFEED_CMS_GRAPHQL_HOST, UNIFEED_CMS_GRAPHQL_PORT, UNIFEED_CMS_GRAPHQL_ENDPOINT, UNIFEED_CMS_GRAPHQL_TOKEN


class DynamicIUHSpider(scrapy.Spider):
    name = "iuh"

    def __init__(self, *args, **kwargs):
        self._graphql_url_endpoint = f'http://{UNIFEED_CMS_GRAPHQL_HOST}:{UNIFEED_CMS_GRAPHQL_PORT}/{UNIFEED_CMS_GRAPHQL_ENDPOINT}'
        self._token = UNIFEED_CMS_GRAPHQL_TOKEN
    
        self.key_max_pages = int(kwargs.pop('key_max_pages', 0)) or None
        self.key_department = kwargs.pop('key_department', None)
        self.key_category = kwargs.pop('key_category', None)

        self.page_counter = {}
        self.latest_dates = {}

        super().__init__(*args, **kwargs)
        
    def get_configs_from_strapi(self):
        query = """
            query {
                crawlerConfigs {
                    url
                    department {
                        key_department
                        department_url
                        department_name
                        categories {
                            key_category
                            category_url
                            category_name
                            last_external_publish_date
                        },
                    },
                    relative_url_list
                    relative_url
                    thumbnail
                    next_pages
                    title
                    content
                    external_publish_date
                }
            }
        """
        res = requests.post(
            self._graphql_url_endpoint,
            json={"query": query},
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self._token}"
            }
        )  
        return res.json()["data"]["crawlerConfigs"]

    def start_requests(self):
        self.configs = self.get_configs_from_strapi()

        for config in self.configs:
            dept = config.get('department', {})
            key_dept = dept.get('key_department')

            if self.key_department and key_dept != self.key_department:
                continue

            for start in config.get('department', {}).get('categories', []):
                if self.key_category and start.get('key_category') != self.key_category:
                    continue

                try:
                    start['last_external_publish_date'] = datetime.strptime(
                        start.get('last_external_publish_date', '2025-05-01'), "%Y-%m-%d"
                    ).date()
                except Exception:
                    start['last_external_publish_date'] = datetime.strptime("2025-05-01", "%Y-%m-%d").date()

                category_url = start['category_url']
                self.page_counter[category_url] = 1

                yield scrapy.Request(
                    url=category_url,
                    callback=self.parse_list,
                    meta={
                        'config': config,
                        'start': start,
                        'category_name': start['category_name'],
                        'department_name': dept.get('department_name', key_dept),
                        'department_url': dept.get('department_url', key_dept),
                        'key_department': key_dept,
                        'key_category': start.get('key_category'),
                        'category_url': category_url,
                        'last_external_publish_date': start['last_external_publish_date']
                    }
                )
    def update_category_last_date(self, key_department, key_category, last_date):
        query_find = """
        query ($key: String!, $dept_key: String!) {
            categories(filters: {
                key_category: { eq: $key },
                department: { key_department: { eq: $dept_key } }
            }) {
                documentId
                last_external_publish_date
            }
        }
        """
        res = requests.post(
            self._graphql_url_endpoint,
            headers={
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {self._token}'
            },
            json={
                'query': query_find,
                'variables': {
                    'key': key_category,
                    'dept_key': key_department
                }
            }
        )
        res_json = res.json()
        data = res_json.get("data", {}).get("categories", [])
        if not data:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y category: {key_department} / {key_category}")
            return
        category = data[0]
        category_id = category['documentId']
        current_last_date_str = category.get('last_external_publish_date')

        # So sÃ¡nh ngÃ y
        if current_last_date_str:
            try:
                current_last_date = datetime.strptime(current_last_date_str, '%Y-%m-%d').date()
                if current_last_date >= last_date:
                    print(f"âš ï¸ KhÃ´ng cáº§n cáº­p nháº­t vÃ¬ ngÃ y hiá»‡n táº¡i ({current_last_date}) >= {last_date}")
                    return
            except Exception as e:
                print(f"âŒ Lá»—i phÃ¢n tÃ­ch ngÃ y hiá»‡n táº¡i: {e}")
        
        # Tiáº¿n hÃ nh cáº­p nháº­t náº¿u khÃ¡c
        mutation = """
        mutation updateCategory($id: ID!, $lastDate: Date!) {
            updateCategory(documentId: $id, data: {
                last_external_publish_date: $lastDate
            }) {
                key_category,
                last_external_publish_date,
                documentId
            }
        }
        """
        res = requests.post(
            self._graphql_url_endpoint,
            headers={
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {self._token}'
            },
            json={
                'query': mutation,
                'variables': {
                    'id': category_id,
                    'lastDate': last_date.strftime('%Y-%m-%d')
                }
            }
        )
        print(f"âœ… Cáº­p nháº­t {key_department}/{key_category} â†’ {last_date.strftime('%Y-%m-%d')}")

    def parse_list(self, response):
        config = response.meta['config']
        category_name = response.meta['category_name']
        category_url = response.meta['category_url']
        key_dept = response.meta['key_department']
        key_cat = response.meta['key_category']
        last_date = response.meta['last_external_publish_date']

        config_key = f"{key_dept}_{key_cat}"
        current_page = self.page_counter.get(category_url, 1)
        self.page_counter[category_url] = current_page + 1

        articles = response.css(config['relative_url_list'])
        should_continue = True
        requests = []

        for article in articles:
            relative_url = article.css(config['relative_url']).get()
            thumbnail = article.css(config['thumbnail']).get()
            date_str = article.css(config['external_publish_date']).get()

            if not date_str:
                self.logger.warning("âŒ KhÃ´ng tÃ¬m tháº¥y ngÃ y Ä‘Äƒng bÃ i viáº¿t")
                continue

            try:
                article_date = datetime.strptime(date_str.strip(), "%d-%m-%Y").date()
            except Exception as e:
                self.logger.warning(f"âŒ Lá»—i parse ngÃ y: {date_str} | {e}")
                continue

            self.logger.info(f"ğŸ“… {relative_url} => {article_date} vs {last_date}")

            if article_date >= last_date:
                # LÆ°u bÃ i má»›i
                if config_key not in self.latest_dates or article_date > self.latest_dates[config_key]:
                    self.latest_dates[config_key] = article_date

                if relative_url:
                    full_url = urljoin(response.url, relative_url)
                    req = scrapy.Request(
                        url=full_url,
                        callback=self.parse_detail,
                        meta={
                            'config': config,
                            'category_name': category_name,
                            'category_url': category_url,
                            'thumbnail': thumbnail,
                            'department_name': response.meta.get('department_name'),
                            'department_url': response.meta.get('department_url'),
                            'external_publish_date': article_date.strftime("%Y-%m-%d")
                        }
                    )
                    requests.append(req)

            else:
                self.logger.info("ğŸ›‘ Táº¥t cáº£ bÃ i trÃªn trang hiá»‡n táº¡i Ä‘á»u Ä‘Ã£ cÅ© â†’ dá»«ng crawler.")
                should_continue = False
                
                # ğŸ” Cáº­p nháº­t last_date lÃªn Strapi náº¿u cÃ³ bÃ i má»›i hÆ¡n
                if config_key in self.latest_dates:
                    new_date = self.latest_dates[config_key]
                    self.update_category_last_date(
                        key_department=key_dept,
                        key_category=key_cat,
                        last_date=new_date
                    )
                
                break

        for req in requests:
            yield req

        # Náº¿u chÆ°a gáº·p bÃ i trÃ¹ng hoáº·c cÅ©, tiáº¿p tá»¥c sang trang tiáº¿p theo
        if should_continue:
            next_pages = response.css(config['next_pages']).getall()
            if next_pages:
                next_page_url = urljoin(response.url, next_pages[-1])
                yield scrapy.Request(
                    url=next_page_url,
                    callback=self.parse_list,
                    meta=response.meta
                )

    def parse_detail(self, response):
        config = response.meta['config']

        item = ArticleItem()
        item['external_url'] = response.url
        item['external_slug'] = response.url.split('/')[-1]
        item['thumbnail'] = response.meta.get('thumbnail')
        item['department_name'] = response.meta.get('department_name')
        item['department_url'] = response.meta.get('department_url')
        item['category_name'] = response.meta['category_name']
        item['category_url'] = response.meta['category_url']
        item['title'] = response.css(config['title']).get()
        item['content'] = response.css(config['content']).get()
        item['external_publish_date'] = response.meta.get('external_publish_date')

        yield item

